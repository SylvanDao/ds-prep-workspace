# SLU13 - Linear Algebra & NumPy, Part 2
In this notebook we will be covering the following:   
- **Matrix multiplication**: multiplication of vectors and matrices, properties of matrix multiplication and transpose, and application in NumPy;
- **The inverse of a matrix**: intuitions behind the inverse, and how to determine it (if it exists!) using NumPy;
- **Additional `numpy.ndarray` methods**: `.min()`, `.max()`, `.sum()` and `.sort()`;
- **(optional section) Systems of linear equations**
- **(optional section) Eigenvalues and eigenvectors**
**What's in this notebook:**

1. [Matrix multiplication](#1.-Matrix-multiplication)

  1.1 [Understanding matrix multiplication](#1.1-Understanding-matrix-multiplication)  
  1.2 [Properties of matrix multiplication](#1.2-Properties-of-matrix-multiplication)  
  1.3 [Transpose multiplication rules](#1.3-Transpose-multiplication-rules)  
  1.4 [Matrix multiplication using NumPy](#1.4-Matrix-multiplication-using-NumPy)


2. [The inverse](#2.-The-inverse)

  2.1 [Finding the inverse of a matrix](#2.1-Finding-the-inverse-of-a-matrix)  
  2.2 [Using NumPy to find the inverse of a matrix](#2.2-Using-NumPy-to-find-the-inverse-of-a-matrix)


3. [Additional NumPy methods](#3.-Additional-NumPy-methods)

  3.1 [`ndarray.max()` and `ndarray.min()`](#3.1-ndarray.max()-and-ndarray.min())  
  3.2 [`ndarray.sort()` and `numpy.sort()`](#3.2-ndarray.sort()-and-numpy.sort())  
  3.3 [`ndarray.sum()`](#3.3-ndarray.sum())
### Imports
import numpy as np
<img src="./media/machine.gif"/>

<br>

<center><i>How this dark meowgic works?!</i> üôÄüòº</center>
---
In SLU12 we learned the most basic concepts of linear algebra: what are vectors, matrices, their basic operations, and applying all that with `ndarrays`.

I promised that if you worked hard enough in SLUs 12 and 13, you'd be able to read the matrix form of the *multiple linear regression* solution:

$$\mathbf{\beta} = (X^TX)^{-1}(X^T\mathbf{y})$$

You can already see that there is some matrix $\mathbf{X}$ and some vectors $\mathbf{\beta}$ and $\mathbf{y}$ inside that equation, as well as the transpose of $\mathbf{X}$. But how would you find the inverse (that exponent $^{-1}$ over there), and how do we multiply all those matrices? All the answers you need will become clear throughout this notebook.

---

‚ö†Ô∏è <i>Although sections 4 and 5 are optional, I suggest you go through them when you have time. Understanding the link between matrices and linear systems, and what are eigenvalues and eigenvectors, are key concepts in data science.</i>

---
<center>You ready?</center>
<center>Let's do this.</center>
---
## 1. Matrix multiplication
Now that we aced dot product and linear combinations in the last SLU, matrix multiplication will be a piece of cake. Oh and remember, matrices are just collections of vectors, column vectors...
### 1.1 Understanding matrix multiplication
#### 1.1.1 General formula for matrix multiplication
<a name="vector_def"></a>
<div class="alert alert-block alert-info">
    The result of the <b>multiplication between a matrix</b> $\mathbf{A}$, of size $m\times n$, <b>and a matrix</b> $\mathbf{B}$, of size $n\times k$, is the matrix $\mathbf{C}$, of size $m\times k$, where each entry $c_{i,j}$ corresponds to the dot product between the row vector in row $i$ of $\mathbf{A}$ and the column vector in column $j$ of $\mathbf{B}$:
    <br>
    <br>
    $$\mathbf{C} = \mathbf{A} \mathbf{B} = 
        \begin{bmatrix}
            c_{1,1} & c_{1,2} & \dots & c_{1,k}\\
            \vdots & \vdots & \ddots & \ddots\\
            c_{m,1} & c_{m,2} & \dots & c_{m,k}
        \end{bmatrix},\;\;
        \text{where}\;
    c_{i,j} = (\text{row }i\text{ in }\mathbf{A})\cdot (\text{column }j\text{ in }\mathbf{B}) = a_{i,1}b_{1,j} + a_{i,2}b_{2,j} + \dots + a_{i,n}b_{n,j}
    $$
    <br>
</div>

Let's go through the formula step by step:

---

**STEP 1**: Compute the dot product between **row 1** in $\mathbf{A}$ and **each column vector** in $\mathbf{B}$:

$\;\;\;\;c_{1,1} = (\text{row 1 in }\mathbf{A}) \cdot (\text{column 1 in }\mathbf{B})$

$\;\,\;\;c_{1,2} = (\text{row 1 in }\mathbf{A}) \cdot (\text{column 2 in }\mathbf{B})$

$\;\;\;\;\vdots$

$\;\;\;\;c_{1,k} = (\text{row 1 in }\mathbf{A}) \cdot (\text{column }k\text{ in }\mathbf{B})$

At this point, we have filled the **first row** in $\mathbf{C}$, as follows:

$\;\;\;(\text{row 1 in }\mathbf{C}) = \begin{bmatrix} c_{1,1} & c_{1,2} & \dots & c_{1,k}\end{bmatrix}$

---

**STEP 2**: Repeat step 1 for **row 2** in $\mathbf{A}$ and **each column vector** in $\mathbf{B}$, until we fill the **second row** in $\mathbf{C}$. 

---

**STEP 3**: **Repeat** the same process for each remaining row of $\mathbf{A}$.

Our last element in matrix $\mathbf{C}$ will correspond to the dot product between the last row vector in $\mathbf{A}$ and the last column vector in $\mathbf{B}$:

$\;\;\;c_{m,k} = (\text{row }m\text{ in }\mathbf{A}) \cdot (\text{column }k\text{ in }\mathbf{B})$

---
That's it. The outcome of multiplying two matrices $\mathbf{A}$ (size $m\times n$) and $\mathbf{B}$ (size $n\times k$) is nothing more than a matrix $\mathbf{C}$ of size $m\times k$ where each element corresponds to the dot product between each row in $\mathbf{A}$ and each column in $\mathbf{B}$:

<br>

$$\mathbf{C} = \mathbf{A}\mathbf{B} = 
        \begin{bmatrix}
            a_{1,1}b_{1,1} + a_{1,2}b_{2,1} + ... + a_{1,n}b_{n,1} & \dots & a_{1,1}b_{1,k} + a_{1,2}b_{2,k} + ... + a_{1,n}b_{n,k}\\
            \vdots & \ddots & \vdots\\
            a_{m,1}b_{1,1} + a_{m,2}b_{2,1} + ... + a_{m,n}b_{n,1} & \dots & a_{m,1}b_{1,k} + a_{m,2}b_{2,k} + ... + a_{m,n}b_{n,k}\\
        \end{bmatrix} =
        \begin{bmatrix}
            c_{1,1} & c_{1,2} & \dots & c_{1,k}\\
            \vdots & \vdots & \ddots & \vdots\\
            c_{m,1} & c_{m,2} & \dots & c_{m,k}
        \end{bmatrix}
        $$

<br>
I'm giving you all that "boring" mathematical notation on purpose. You might see a lot of it when reading about algorithms and other mathematical topics. üòâ
---
**Two important notes**

‚ùóÔ∏è**1)** Remember you can **only** multiply $\mathbf{A}$ by $\mathbf{B}$ if the number of columns in $\mathbf{A}$ is equal to the number of rows in $\mathbf{B}$. It's easy to see why. Just remember that the dot product cannot be calculated for vectors of different dimensions.

The output matrix will always have the same number of rows as the first matrix (or vector) and the same number of columns as the second matrix (or vector).

If you calculate the matrix product between a $1\times n$ vector and a $n\times 1$ vector, you get the dot product between vectors you learned in SLU12 (scalar product). If you calculate the matrix product between a $n\times 1$ vector and a $1\times n$ vector, you get an $n\times n$ square matrix.

‚ùóÔ∏è**2)** The order in matrix multiplication matters!

If we consider two matrices $\mathbf{A}$ and $\mathbf{B}$, both of which are $n\times n$, then the product $\mathbf{A}\mathbf{B}$ will, in general, result in a different result from $\mathbf{B}\mathbf{A}$ (equality between both of these products will only happen in special cases). We encourage you to choose two matrices and test this property by yourself.
---
#### 1.1.2 Example:


What's the result of multiplying $\mathbf{A}$ by $\mathbf{B}$, knowing that $\mathbf{A} = \begin{bmatrix} -1 & 2\\ 4 & 6\end{bmatrix}$ and $\mathbf{B} = \begin{bmatrix} 0 & 3 & 6\\ -2 & 0 & -1\end{bmatrix}$?

Let's call $\mathbf{M}$, for "Mathmagic", to the output of our matrix multiplication, $\mathbf{M} = \mathbf{A}\mathbf{B}$. Because $\mathbf{A}$ is of size $2\times 2$ and $\mathbf{B}$ is of size $2\times 3$, the resulting "mathmagical" matrix $\mathbf{M}$ will have size $2\times 3$.

You can fill the matrix step by step, using the formula with the dot products:
<img src="./media/matrix_multiplication.PNG" width="360"/>

Or just fill the dot products on the output matrix all at once:

$$\mathbf{M} = 
    \begin{bmatrix}
        -1\times 0 + 2\cdot (-2)  &  -1\times 3 + 2\times 0  &  -1\times 6 + 2\times (-1)\\
        4\times 0 + 6\cdot (-2)  &  4\times 3 + 6\times 0  &  4\times 6 + 6\times (-1)
    \end{bmatrix} =
    \begin{bmatrix}
        -4 & -3 & -8\\
        -12 & 12 & 18
    \end{bmatrix}
$$
---
<img src="./media/what_if.jpg" width="500"/>
---
#### 1.1.3 Matrix multiplication and linear combinations
You should remember from SLU12 that linear combinations are things like $c\cdot \mathbf{u} + d\cdot \mathbf{v}$, where $\mathbf{u}$ and $\mathbf{v}$ are vectors, and $c$ and $d$ are real numbers (scalars).

Consider the matrices 
$\mathbf{A} = 
    \begin{bmatrix}
        a_{1,1} & a_{1,2}\\
        a_{2,1} & a_{2,2}\\
    \end{bmatrix}
$
and
$\mathbf{B} = 
    \begin{bmatrix}
        b_{1,1} & b_{1,2}\\
        b_{2,1} & b_{2,2}\\
    \end{bmatrix}
$.

---

```
Instructor: - "Can we multiply A by B?"
Student: - "Yes!"
Instructor: - "Why?"
Student: - "Because the number of columns in A is equal to the number of rows in B!
Instructor: - "Precisely!"
```

---
We already know that one way we could multiply $\mathbf{A}$ by $\mathbf{B}$ is to compute the dot product between each row vector in $\mathbf{A}$ and each column vector in $\mathbf{B}$ .

Think about column 1 of matrix $\mathbf{B}$. Let's say we're very tired and just want to get the first column of $\mathbf{B}$ before taking a nap.

So let's pretend we lost the entries in column 2:

$
    \begin{bmatrix}
        a_{1,1} & a_{1,2}\\
        a_{2,1} & a_{2,2}\\
    \end{bmatrix}
    \begin{bmatrix}
        b_{1,1} & ?\\
        b_{2,1} & ?\\
    \end{bmatrix}
$

We'll replace every value we can't determine without the second column of $\mathbf{B}$, by a question mark, $?$.

What part of the final matrix can we fill with the available column?

$
    \begin{bmatrix}
        a_{1,1} & a_{1,2}\\
        a_{2,1} & a_{2,2}\\
    \end{bmatrix}
    \begin{bmatrix}
        b_{1,1} & ?\\
        b_{2,1} & ?\\
    \end{bmatrix} = 
    \begin{bmatrix}
        a_{1,1}b_{1,1} + a_{1,2}b_{2,1} & a_{1,1} ? + a_{1,2} ?\\
        a_{2,1}b_{1,1} + a_{2,2}b_{2,1} & a_{2,1} ? + a_{2,2} ?\\
    \end{bmatrix} = 
    \begin{bmatrix}
        a_{1,1}b_{1,1} + a_{1,2}b_{2,1} & ?\\
        a_{2,1}b_{1,1} + a_{2,2}b_{2,1} & ?\\
    \end{bmatrix}\;\;\;\;(1)
$
---
Using only the first column in $\mathbf{B}$, we were able to fill the first column of the output matrix. This is the same as having multiplied $\mathbf{A}$ by the column vector $\begin{bmatrix}b_{1,1}\\ b_{2,1}\end{bmatrix}$.

But what's really interesting is that if we had used only the second column of $\mathbf{B}$ we would have filled only the second column in the output matrix. üòÆ

What's actually happening in the output matrix is that we're creating **linear combinations** of the columns of $\mathbf{A}$.

Notice that we can rearrange the resulting column as follows:

$b_{1,1}a_{1,1} + b_{2,1}a_{1,2}$

$b_{1,1}a_{2,1} + b_{2,1}a_{2,2}$
If we consider the column vectors of $\mathbf{A}$, $\begin{bmatrix}a_{1,1}\\ a_{2,1}\end{bmatrix}$ and $\begin{bmatrix}a_{1,2}\\ a_{2,2}\end{bmatrix}$, we can see that what we're doing is simply summing $b_{1,1}\cdot \begin{bmatrix}a_{1,1}\\ a_{2,1}\end{bmatrix}$ with $b_{2,1}\cdot \begin{bmatrix}a_{1,2}\\ a_{2,2}\end{bmatrix}$ yielding:

$b_{1,1}\cdot \begin{bmatrix}a_{1,1}\\ a_{2,1}\end{bmatrix} + b_{2,1}\cdot \begin{bmatrix}a_{1,2}\\ a_{2,2}\end{bmatrix}$

which gets us back to the first column $(1)$ in our result matrix:

$
    \begin{bmatrix}
        a_{1,1}b_{1,1} + a_{1,2}b_{2,1}\\
        a_{2,1}b_{1,1} + a_{2,2}b_{2,1}\\
    \end{bmatrix}
$
---
So, when multiplying a matrix $\mathbf{A}$ by a matrix $\mathbf{B}$, you get a matrix where each column is a linear combination of the column vectors of $\mathbf{A}$, using the scalars of each column in $\mathbf{B}$.
---
<center>Generally speaking, the columns of the matrix product $\mathbf{A}\mathbf{B}$ are linear combinations of the columns of the first matrix, $\mathbf{A}$.</center>

<center>Similarly, the rows of the product $\mathbf{A}\mathbf{B}$ are linear combinations of the rows of the second matrix, $\mathbf{B}$.</center>
<br>

<img src="./media/surprised.gif" width="350"/>

<br>

<center>Take a moment to meditate on that, before moving on.</center>

<br>

<center>Oh, and here's a <a href="https://eli.thegreenplace.net/2015/visualizing-matrix-multiplication-as-a-linear-combination/">visual explanation</a> that might help you.</center>
---
> üìù **Pen and paper exercise 1**: Grab a pen and a piece of paper and determine $\begin{bmatrix} 0\\ 1\end{bmatrix}\begin{bmatrix}-1 & 2\end{bmatrix}$
### 1.2 Properties of matrix multiplication
Don't worry about memorizing all those properties, just check them and save them for your reference:

$\;\;\text{1. }\;\; \mathbf{A}(\mathbf{B}\mathbf{C}) = (\mathbf{A}\mathbf{B})\mathbf{C}\;\;$ **(Associativity)**

$\;\;\text{2. }\;\; \mathbf{A}(\mathbf{B}\pm \mathbf{C}) = \mathbf{A}\mathbf{B} \pm \mathbf{A}\mathbf{C}\;\;$ **(Distributivity)**

$\;\;\text{3. }\;\; (\mathbf{A}\pm \mathbf{B})\mathbf{C} = \mathbf{A}\mathbf{C} \pm \mathbf{B}\mathbf{C}\;\;$ **(Distributivity)**

$\;\;\text{4. }\;\; c(\mathbf{A}\mathbf{B}) = (c\mathbf{A})\mathbf{B}\;\;$ **(Associativity of scalar)**

$\;\;\text{5. }\;\; \mathbf{A}\mathbf{0} = \mathbf{0}$ and $\mathbf{0}\mathbf{B} = \mathbf{0}\;\;$ **(Multiplicative Property of Null Matrix)**

$\;\;\text{6. }\;\; \mathbf{A}\mathbf{I} = \mathbf{A}$ and $\mathbf{I}\mathbf{A} = \mathbf{A}\;\;$ **(Multiplicative Identity)**

where $\mathbf{I}$ is the identity matrix and $\mathbf{0}$ is the zero (or null) matrix, just like we learned in SLU12. $c$ is a scalar.
> üìù **Pen and paper exercise 2**: Multiply $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ by $\begin{bmatrix}-1 & 2\\6 & 0\end{bmatrix}$. Do you get the expected result as by property 6?
>
>  Look at the rows of the second matrix and remember that the resulting matrix rows are linear combinations of the second matrix rows. Notice how the disposition of scalars $1$ and $0$ inside the identity matrix allows you to maintain the row vectors of the second matrix unchanged.
>
> Again, it's "mathmagic". All you've learned is connected.
### 1.3 Transpose multiplication rules
We've seen the transpose laws for addition and multiplication by a scalar in the last SLU. There are also special rules for matrix multiplication with the transpose:

$\;\;\text{1. }\;\; (\mathbf{A}\pm \mathbf{B})^T = \mathbf{A}^T\pm \mathbf{B}^T$

$\;\;\text{2. }\;\; (c\mathbf{A})^T = c\mathbf{A}^T$

$\;\;\text{3. }\;\; (\mathbf{A} \mathbf{B})^T = \mathbf{B}^T \mathbf{A}^T$

$\;\;\text{4. }\;\; (\mathbf{A}^T)^T = \mathbf{A}$
> üìå **Tip**: These will become very handy when reading machine learning algorithms equations, and especially some mathematical "tricks" that are applied to get to a simple matrix form solution.
### 1.4 Matrix multiplication using NumPy
#### 1.4.1 Multiplying matrices using `numpy.matmul()`
As always, NumPy has got us covered. We can multiply two matrices using `np.matmul()`:
# bring back matrices from our previous example above
# 2D array to represent A
A = np.array([[-1, 2],
              [4, 6]])
# 2D array to represent B
B = np.array([[0, 3, 6],
              [-2, 0, -1]])

# numpy.matmul() to multiply A by B
C = np.matmul(A, B)
C
If the resulting matrix above does not match the one we obtained in [our previous example](#1.1.2-Example:), you have a bad instructor.
On the [reference page for `numpy.matmul()`](https://numpy.org/doc/1.20/reference/generated/numpy.matmul.html) we can read the following on the "Notes" section:

<img src="./media/matmul.png" width="700"/>
There is a lot of info in there, but the key goal here is that you understand that when using 1-D arrays (which could happen if you're representing a vector, as we've seen in SLU12), the behaviour of the function will change. Let's check some examples:
**Using 2D arrays:**
# multiplying (2x1) column vector by (1x2) row vector using 2D arrays
np.matmul(np.array([[0],
                    [1]]),  # 2D array
          np.array([[-1, 2]]))  # 2D array
**Using 1D arrays:**
Notice that when you use 1D arrays to represent vectors, NumPy cannot guess whether it's dealing with row or column vectors!! It will return the dot product:
# multiplying (2x1) column vector by (1x2) row vector using 2D array and 1D array, respectively
np.matmul(np.array([0, 1]),  # 1D array
          np.array([-1, 2]))  # 1D array
**Using 2D and 1D arrays:**
As you can see below, although at first sight it looks like we're doing the right thing (multiplying column vector of dimension $2\times 1$ by row vector of dimension $1\times 2$), NumPy will get confused:
x = np.array([[0],[1]])  # 2D array, matrix 2x1 (column vector)
y = np.array([-1, 2])  # 1D array, row vector 1x2
try:
    np.matmul(x, y)
except Exception as e: 
    print("ValueError:", e)
As we can read from the documentation page of `numpy.matmul()`:

> If the **second argument** is 1-D, it is promoted to a matrix by **appending** a 1 to its dimensions. After matrix multiplication the appended 1 is removed.

In this case, the shape of our second argument is `(2,)`, so it will be transformed under the hood to `(2, 1)`, which is not compatible with `(2, 1)` (the shape of our first argument) for matrix multiplication.
print("x.shape:", x.shape)
print("y.shape:", y.shape)
On the other hand, multiplying in the reverse order (`y` as first argument and `x` as second) will work. In this case, our first argument has shape `(2,)`, which will be converted to `(1,2)`, according to NumPy:

> If the **first argument** is 1-D, it is promoted to a matrix by **prepending** a 1 to its dimensions. After matrix multiplication the prepended 1 is removed.

Hence, NumPy will multiply successfully the `(1,2)` shaped array by the `(2,1)` shaped array (our second argument, `x`):
np.matmul(y, x)
Yes, I know, it's a bit confusing, but it's the price we have to pay to have one awesome library that does all the computations for us. It's worth the effort. üòå
> üìå **Tip**: Just like before, notice how important the concept of array dimensions and shape is when dealing with NumPy arrays.
‚ö†Ô∏è **The `*` operator is not to be used for matrix multiplication!!**
# 2D array to represent A
A = np.array([[-1, 2],
              [4, 6]])
# 2D array to represent B
B = np.array([[0, 3],
              [-2, 0]])
# matrix multiplication
np.matmul(A, B)
The `*` when used between matrices (2D arrays) will be interpreted as element-wise multiplication and not matrix multiplication.
# element-wise multiplication
A * B
#### 1.4.2 Using `numpy.matmul()` versus `numpy.dot()`
Remember `numpy.dot()`, which we used to determine the scalar product (or dot product) between two vectors in the last SLU?

Well, we could use `numpy.dot()` as well, to perform the multiplication between two matrices.
np.dot(A, B)
NumPy advises you, **however**, to use `numpy.matmul()` in the case of 2-D arrays, as you can see in the documentation page of [`numpy.dot()`](https://numpy.org/doc/1.20/reference/generated/numpy.dot.html):

<img src="./media/dot_vs_matmul.png" width="600"/>
## 2. The inverse
The first question to ask yourself is: __"If a given matrix is square, is it invertible or not?"__
---
<img src="./media/best_question.gif" width="320"/>
---
A square matrix **may or may not be invertible**. It all (linearly) "depends".

So, here's the general definition of the inverse:
<a name="vector_def"></a>
<div class="alert alert-block alert-info">
    <b>A matrix</b> $\mathbf{A}$, of size $m\times m$, <b>is invertible, if there is a matrix</b> $\mathbf{B}$ such that $\mathbf{A}\mathbf{B} = \mathbf{B}\mathbf{A} = \mathbf{I}_m$. When it exists, $B$ is unique, is called the inverse of $\mathbf{A}$, and is denoted by $\mathbf{A}^{-1}$.
    <br>
    <br>
    If one can find a square matrix $\mathbf{B}$ that satisfies $\mathbf{B}\mathbf{A} = \mathbf{I}_m$, then $\mathbf{A}\mathbf{B} = \mathbf{I}_m$, that is, $\mathbf{B} = \mathbf{A}^{-1}$, and vice-versa.
</div>

### 2.1 Finding the inverse of a matrix
Bring to your memory (or go check that again) all you've learned so far on:
- linear independence of vectors; (SLU12)
- matrix multiplication in terms of linear combinations of columns. (SLU13 - this one!)

Once again, everything is interconnected.
---
Let's check a very simple example. Suppose you have a $2\times 2$ matrix $\mathbf{A}$:

$$\mathbf{A} = 
    \begin{bmatrix}
        1 & 2\\
        2 & 4
    \end{bmatrix}
$$


```
Instructor: -"Dear student, what's the inverse of A?"
Student: -"I'm sorry but I can't tell you yet. The correct question is: A being square, is it invertible or not?"
Instructor: -"That's the best answer of all!"
```

We know that **if the square matrix** $\mathbf{A}$, of size $2\times 2$, is invertible, then we should be able to find $\mathbf{B}$ such that $\mathbf{B}\mathbf{A} = I_2$, or $\mathbf{A}\mathbf{B} = I_2$ ($2\times 2$ identity matrix).

Let's represent that as:

$$\begin{bmatrix}
       1 & 2\\
       2 & 4
   \end{bmatrix}
   \begin{bmatrix}
       b_{1,1} & b_{1,2}\\
       b_{2,1} & b_{2,2}
   \end{bmatrix} \stackrel{?}{=}
   \begin{bmatrix}
       1 & 0\\
       0 & 1
   \end{bmatrix}
$$

<br>

Recall that "*__The columns of the product of two matrices__ are __combinations of the columns__ of the __first matrix.__*" If you're clueless, [reread section 1.1.3](#1.1.3-Matrix-multiplication-and-linear-combinations).

With that in mind, we can write the **first column** of $\mathbf{A} \mathbf{B}$ as the linear combination:

$$
b_{1,1}\cdot
\begin{bmatrix}
    1\\
    2
\end{bmatrix}
+
b_{2,1}\cdot
\begin{bmatrix}
    2\\ 
    4
\end{bmatrix}
$$

and the second as the linear combination:

$$
b_{1,2}\cdot
\begin{bmatrix}
    1\\
    2
\end{bmatrix}
+
b_{2,2}\cdot
\begin{bmatrix}
    2\\ 
    4
\end{bmatrix}
$$
**If the inverse exists**, we should be able to write:

$$
b_{1,1}\cdot
\begin{bmatrix}
    1\\
    2
\end{bmatrix}
+
b_{2,1}\cdot
\begin{bmatrix}
    2\\ 
    4
\end{bmatrix} = 
\begin{bmatrix}
    1\\ 
    0
\end{bmatrix}
$$

and:

$$
b_{1,2}\cdot
\begin{bmatrix}
    1\\
    2
\end{bmatrix}
+
b_{2,2}\cdot
\begin{bmatrix}
    2\\ 
    4
\end{bmatrix} =
\begin{bmatrix}
    0\\ 
    1
\end{bmatrix}
$$
Notice something familiar there?...

The columns of $\mathbf{A}$ are actually two collinear vectors. If you multiply the first column by scalar $2$, you get the second column of $\mathbf{A}$.

Remember that if you have 2 collinear vectors in a 2D space, you'll get "stuck on the line"?
---

```Python
if student.current_thought == "I have no idea what she's talking about.":
    print("You just made your instructor shed a tear. Please reread Learning Notebook 1 of SLU12, section on linear independence.")

elif student.current_thought == "I know what you're talking about!!":
    print("2 collinear 2D vectors cannot define the space of all 2D vectors; but 2 non-collinear 2D vectors can!")

else:
    print("Hello, World!")
```

---
The concept of linear (in)dependence is extremely important. Here we are, just about to use it again.

Applying the same concept to our $\mathbf{A}$ matrix columns, we see that it is **impossible** to find some matrix $\mathbf{B}$ that multiplied by $\mathbf{A}$ (which has 2 collinear columns) would yield two noncollinear vector columns, namely $\begin{bmatrix} 1\\ 0\end{bmatrix}$ and $\begin{bmatrix} 0\\ 1\end{bmatrix}$, out of that same line.

> üìå **Tip**: Notice that each and every identity matrix is made up of $n$ vectors, each vector being $n$-dimensional, which are all orthogonal to each other. Since they're not collinear, we can always find a linear combination with those $n$ vectors to represent any $n$-dimensional vector we want.

> *And that's why my favorite matrix is the identity.*
---
<img src="./media/identity_matrix.png"/>
---
‚ùóÔ∏è **Key takeaway**: A square matrix is invertible **if and only if** none of its columns is a linear combination of the others.

‚ùóÔ∏è **Singular matrix:** A square matrix that is **not invertible** is called a **singular matrix**.
> üìù **Pen and paper exercise 3**: Consider the matrix  $\;\;\mathbf{A} = \begin{bmatrix} 1 & 0 & 1\\ 0 & 1 & 1\\ 3 & -2 & a_{3,3}\end{bmatrix}$. Find **the value** $a_{3,3}$ for which $\mathbf{A}$ is **non-invertible** (singular). Do you think you could choose more than 1 value?
Let's call our geeky friend NumPy to determine the inverse for us, because our brains need a break.
---
### 2.2 Using NumPy to find the inverse of a matrix
*NumPy, dear NumPy, will you do the math for us?...*
Remember [`numpy.linalg`](https://numpy.org/doc/1.20/reference/routines.linalg.html) module?

Well, we can use it to determine the inverse of a matrix, with something called [`numpy.linalg.inv()`](https://numpy.org/doc/1.20/reference/generated/numpy.linalg.inv.html). Let's check it out.
Trying to determine the inverse of a non-invertible square matrix will result in an error, more specifically, a `LinAlgError` with the message `Singular matrix`. (*makes sense, right?*)
# a non-invertible square matrix
A = np.array([[1, 2],
              [2, 4]])
A

# try to determine the inverse using NumPy
try:
    np.linalg.inv(A)
except Exception as e:  # NumPy is smart, so it throws an error because A is not invertible
    print("LinAlgError:", e)
Now let's try to determine the inverse of an invertible matrix using NumPy:
# an invertible matrix
B = np.array([[0, 2],
              [-4, 1]])
B_inv = np.linalg.inv(B)
B_inv
Thank you, NumPy!!
Finally, for the skeptics (*I know you're out there*), let's check that multiplying `B` by `B_inv` actually yields the identity! Oh and by the way, this will not always be the case because NumPy approximates the numbers (*no computer has infinite memory!*), thus in some cases you could for example get a diagonal with values just slightly different from 1.
# multiply B by B_inv
print("B multiplied by B_inv:\n", np.matmul(B, B_inv))

# multiply B_inv by B (because we're paranoid and insane, and doubt the Mathematicians)
print("B_inv multiplied by B:\n", np.matmul(B_inv, B))
---
```
All this questioning just invoked Gauss, Newton, Cayley, Hamilton and their fellow dead Mathemagicians from their graves:
-"How dare you doubt our findings on the workings of the inverse? You Math wannabes!!"
```
<img src="./media/mathmagicians.png"/>
```Python
    (we will now shamefully run away to the following section)
```
---
By the way, the inverse of a matrix was the last missing piece we needed to be able to read the matrix form of the multiple linear regression solution:

$$\mathbf{\beta} = (X^TX)^{-1}(X^T\mathbf{y})$$

<br>

<center>üò≤üò±üò≤üò±</center>
---
## 3. Additional NumPy methods
---
When it comes to the `ndarray` class, there are actually some pretty handy methods you can use to reorganize your data and calculate some metrics.

Let's meet these buddies.
### 3.1 `ndarray.max()` and `ndarray.min()`
The method [`ndarray.max()`](https://numpy.org/doc/1.20/reference/generated/numpy.ndarray.max.html) allows you to return the maximum value of an array **along** a chosen axis:
# a 3x3 square matrix
A = np.array([[1, -1, 0],
              [2, -2, 3],
              [6, 0, -1]])
A
# using axis=1 we get the maximum value per row of A
A.max(axis=1)
# using axis=0 we get the maximum value per column of A
A.max(axis=0)
# if we don't specify the axis, it will use the default value axis=None (for v1.26 of NumPy)
# meaning it will return the maximum value in the entire 2D array
A.max()
The method [`ndarray.min()`](https://numpy.org/doc/1.20/reference/generated/numpy.ndarray.min.html) works in a similar manner, but instead of returning maximum values it returns minimum values:
A  # remember A
# using axis=1 we get the minimum value per row of A
A.min(axis=1)
# using axis=0 we get the minimum value per column of A
A.min(axis=0)
# if we don't specify the axis, it will use the default value axis=None (for v1.26 of NumPy)
# meaning it will return the minimum value in the entire 2D array
A.min()
```
Student: -"What if we have repeated values?"
Instructor: -"No idea!! Let's test it out!!"
```
Let's see what would happen with a 2D array `A` that has two maximum values (number `3`) on the last column:
# 2D array with 2 equal maximum values in 3rd colum
A = np.array([[1, -1, 3],
              [2, 2, 3],
              [6, 0, -1]])

# using axis=0 we get the maximum value per column of A
A.max(axis=0)
NumPy chooses one of the repeated values.

```
Student: -"Ok, cool!"
```
### 3.2 `ndarray.sort()` and `numpy.sort()`
The method [`ndarray.sort()`](https://numpy.org/doc/1.20/reference/generated/numpy.ndarray.sort.html) allows us to sort an array **in-place**. We also need to choose the axis along which to sort:
A
A.sort(axis=1)  # this will change the array in-place!!
A
Each row was sorted in ascending order from the lowest to the highest value.

‚ùóÔ∏è **Notice that** we lose the integrity of the matrix (each row is sorted independently of the remainder, so our columns might change too!).
‚ùóÔ∏è Also note that NumPy **changed your array** without you explicitly having to assign the sorted array to a certain variable!! Be careful when using this method.
**If you don't want your array to be changed in-place**, you can use [numpy.sort()](https://numpy.org/doc/1.20/reference/generated/numpy.sort.html) instead:
A = np.array([[1, -1, 3],
              [2, 2, 3],
              [6, 0, -1]])
np.sort(A) # sort along the last axis (default is axis=-1)
np.sort(A, axis=None)  # sort the flattened array (array reshaped to 1 dimension)
np.sort(A, axis=0)  # sort along the first axis
If you wanted to sort in reverse order, you would have to do a *mathmagical trick* like the following:
-np.sort(-A, axis=0)  # sort along the first axis
‚ùóÔ∏è NumPy is cool and all, but sometimes it can be very tricky... Remember that snippet, it will probably be useful.
### 3.3 `ndarray.sum()`
Finally, it is quite useful to find the sum of a matrix columns and/or rows. We can do that with the method [`ndarray.sum()`](https://numpy.org/doc/1.20/reference/generated/numpy.ndarray.sum.html).
# recall A 'cause we have bad memory
A
# The default, axis=None, will sum all of the elements of the input array
print(A.sum())
B = np.array([[[1], [2]], [[3], [4]]])
B.ndim
B.sum()
A.sum(axis=0)  # Sum elements along axis=0 (sum elements in each column)
A.sum(axis=1)  # Sum elements along axis=1 (sum elements in each row)
---
<center>Feeling confident about linear algebra? I hope so!</center>

<img src="./media/we_got_this.gif"/>

<br>

You might want to take a break now. Do some breathing exercises, eat a chocolate, get some rest...

I know I will.
---
## Time to make a choice...
Do you wish to dive deeper into linear algebra?

---

<img src="./media/choice.jpg" width="800"/>

---
üî¥ [Click here to **discover the workings of the matrix**.](#4.-Systems-of-Linear-Equations) (optional sections)


üîµ [Click here to **use NumPy in "blissful" ignorance**.](#Reading-the-matrix-form-of-the-multiple-linear-regression-solution)
---












































<center>Up up up!! Choose you must.</center>






















































---
## 4. Systems of Linear Equations
### (optional section)
---
You chose well.

<img src="./media/proud.gif"/>
---
### 4.1 School days
Remember studying systems of linear equations in Maths class? A simple linear system would look something like this:

$\left\{
    \begin{align*}
        x_{1} + 2x_{2} = 5\\ 
        2x_{1} - x_{2} = 2\\ 
    \end{align*}
\right.$

I know, you probably used to see an $x$ and a $y$ there. But using this notation will make things much clearer when we jump into the matrix form.

How did you learn to [solve this at school](https://www.mathsisfun.com/algebra/systems-linear-equations.html)? The way I learned it was to use the [substitution method](https://www.khanacademy.org/math/algebra-home/alg-system-of-equations/alg-solving-systems-of-equations-with-substitution/v/solving-linear-systems-by-substitution). It would look something like this:

**Old school step 1** - Use the first equation to solve for $x_1$:

$x_1 = 5 - 2x_2$

**Old school step 2** - Replace (*substitute*) $x_1$ in the second equation for $5 - 2x_2$ and find the value of $x_2$:

$2\times(5 - 2x_2) - x_2 = 2 \iff x_2 = \frac{8}{5}$

**Old school step 3** - Now you would just replace the value you found for $x_2$ in the first equation, and get $x_1 = \frac{9}{5}$.

Don't you feel nostalgic right now?
---
> üìù **Pen and paper exercise 4**: Draw the lines described by the 2 equations of our linear system on the xy-plane; check that they intersect.
> 
> If they didn't intersect, meaning they would be parallel, you would have two possibilities:
> - The system has no solution, meaning that the lines are parallel and distinct, **or**
> - The lines would coincide, that is, the two equations would represent the same line, thus the system would have infinite solutions.
### 4.2 Using matrices to represent systems of linear equations
We can represent our system:

$\left\{
    \begin{align*}
        x_1 + 2x_2 = 5\\ 
        2x_1 - x_2 = 2\\ 
    \end{align*}
\right.$

as a multiplication of a matrix $\mathbf{A}$ by a vector $\mathbf{x}$, as follows:

$\begin{bmatrix}
    1 & 2\\
    2 & -1
\end{bmatrix}
\begin{bmatrix}
    x_1\\
    x_2
\end{bmatrix} = 
\begin{bmatrix}
    5\\
    2
\end{bmatrix}
$

If you don't believe me, compute the matrix product on the left side of the equation, and check that you get the same equations of our linear system.
---
**Bridge to linear regression:**

In very simplistic terms, you can think of the matrix formulation of the linear regression as $\mathbf{y} = \mathbf{X}\mathbf{b}$, where you use your matrix input data, $\mathbf{X}$, and your known outcome values, $\mathbf{y}$, to find the **unknown** vector of coefficients, $\mathbf{b}$. Once you find $\mathbf{b}$, you can predict $\mathbf{y}$ for any new value of $\mathbf{X}$

‚ùóÔ∏è **Notice that the notation is different** from ours. In linear regression, $\mathbf{X}$ corresponds to $\mathbf{A}$, $\mathbf{b}$ to our unknown values $\mathbf{x}$, and $\mathbf{y}$ to $\mathbf{b}$.

---

‚ùóÔ∏è *In "real life", we can't build "simple and perfect" systems of linear equations, like we do here, but understanding the basics will give you a good starting point.*

---
When you actually start looking into the Maths behind machine learning algorithms, you'll probably think to yourself:

"*I wish Calculus and Statistics were as easy as Linear Algebra. (sad face)*"
---
<center><i>Did someone just say 'Calculus' and 'Statistics'???</i></center>
    
<img src="./media/scream.gif" width="400"/>
    
<br>
<br>
    
<center>Yep, sooner or later, they will find you and they will haunt you. For now, let's just focus on strengthening your linear algebra skills.</center>
---
### 4.3 Solving systems of linear equations with Gauss-Jordan elimination
Remember that the matrix product $\mathbf{A}\mathbf{x}$ is a **linear combination of the columns in $\mathbf{A}$**:

$$\mathbf{b} = \begin{bmatrix}
    a_{1,1}x_1 + a_{1,2}x_2 + ... + a_{1,n} x_n\\
    a_{2,1}x_1 + a_{2,2}x_2 + ... + a_{2,n} x_n\\
    \vdots\\
    a_{m,1}x_1 + a_{m,2}x_2 + ... + a_{m,n} x_n\\
\end{bmatrix} = x_1
\begin{bmatrix}
    a_{1,1}\\
    a_{2,1}\\
    \vdots\\
    a_{m,1}\\
\end{bmatrix} + x_2
\begin{bmatrix}
    a_{1,2}\\
    a_{2,2}\\
    \vdots\\
    a_{m,2}\\
\end{bmatrix} + ... + x_n
\begin{bmatrix}
    a_{1,n}\\
    a_{2,n}\\
    \vdots\\
    a_{m,n}\\
\end{bmatrix}
$$
> üìå **Tip**: __If matrix $\mathbf{A}$ is a square matrix ($m=n$) and is invertible__, we can always find an $\mathbf{x}$ for any given $\mathbf{b}$.
> 
> It's just about determining $\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$ (we'll get there).
You might remember that the **3 possible outcomes of a linear system** are:

1. the system has **no solution** (also called an **inconsistent** system);
2. the system has **a unique solution**;
3. the system has **an infinite number of solutions**;
#### 4.3.1 Walkthrough example
Consider the following system of linear equations (3 equations, 3 unknowns):

$\left\{
    \begin{align*}
        x_1 + 2x_2 + 3x_3 = 2\\ 
        4x_1 + x_2 + 2x_3 = 3\\ 
        x_1 + x_2 + x_3 = 1
    \end{align*}
\right.$

We can write it in matrix form as
$\;\;\mathbf{A}\mathbf{x} = \mathbf{b} \iff \begin{bmatrix}
    1 & 2 & 3\\
    4 & 1 & 2\\
    1 & 1 & 1\\
\end{bmatrix}
\begin{bmatrix}
    x_1\\
    x_2\\
    x_3
\end{bmatrix} = 
\begin{bmatrix}
    2\\
    3\\
    1
\end{bmatrix}
$.
```
Instructor: -"What do you think? Will this system have (i) no solution, (ii) a unique solution or (iii) an infinite number of solutions?"
Student: -"No idea..."
Instructor: -"Haha me neither! Just made those numbers up, so let's check it out..."
```
**Gauss-Jordan elimination:**

Gauss-Jordan elimination method is a method to solve a linear system in matrix form, using an augmented matrix, $\mathbf{A} | \mathbf{b}$, and changing it within a given set of rules, until we get to the **reduced row echelon form**, where **all entries below the diagonal are zeros and the first nonzero number in each row is $1$**.

If we can get to such a form, and if our matrix is square, then it means we have a unique solution, and we can continue reducing the left matrix to the identity, getting the solution vector on the right side. Let's use this in our example.
---
**Step 1** - represent our system by an augmented matrix:

$\mathbf{A}|\mathbf{b} = \left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    4 & 1 & 2 & 3\\
    1 & 1 & 1 & 1
\end{array}\right]$

#### Elementary row operations (EROs):

Our goal is to reach the **reduced row echelon form**. For that we can operate on our matrix $\mathbf{A}$, using any number of the 3 elementary row operations:

- **ERO 1**: Interchange any two equations (rows in the augmented matrix);

- **ERO 2**: Multiply any equation (row) by a **nonzero** constant;

- **ERO 3**: Add a multiple of one equation (row) to another.

These rules are all just basic arithmetics you can perform on equations, without changing the system's solution (if it exists).
**Step 2** - find the reduced row echelon form, using elementary row operations (ERO):

Because we like columns, let's perform the elimination column by column. If you look at our augmented matrix, we already have a $1$ as the first element in the first entry of the diagonal. Going along the first column, how can we reduce that $4$ into a $0$, using EROs?

We could, for example, add the first row ($R_1$), multiplied by $-4$, to the second row ($R_2$):

$R_2 - 4 R_1 = \left[\begin{array}{ccc|c}4 & 1 & 2 & 3\end{array}\right] - 4\cdot \left[\begin{array}{ccc|c}1 & 2 & 3 & 2\end{array}\right] = \left[\begin{array}{ccc|c}0 & -7 & -10 & -5\end{array}\right]$

which we represent as:

$\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    4 & 1 & 2 & 3\\
    1 & 1 & 1 & 1
\end{array}\right]\xrightarrow{R_2 - 4 R_1 \to R_2}
\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & -7 & -10 & -5\\
    1 & 1 & 1 & 1
\end{array}\right]$
Let's finish reducing column 1, using a similar strategy:

$\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & -7 & -10 & -5\\
    1 & 1 & 1 & 1
\end{array}\right]\xrightarrow{R_3 - R_1 \to R_3}
\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & -7 & -10 & -5\\
    0 & -1 & -2 & -1
\end{array}\right]
$
We're finished reducing our first column!

Let's now go to the second column. Remember we want to get to a matrix where all entries below the diagonal are zeros and all entries in the diagonal are ones. For that we need to reduce that $-7$ in the diagonal of column 2, to $1$. Take a moment to think about which row could you use to do that, without messing up with column 1.

That's right, we can only use the third row. If we used the first, we would "undo" our work for the first column!

$\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & -7 & -10 & -5\\
    0 & -1 & -2 & -1
\end{array}\right]\xrightarrow{R_2 - 8 R_3 \to R_2}
\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & 1 & 6 & 3\\
    0 & -1 & -2 & -1
\end{array}\right]$
By now, you would reduce that $-1$ at the end of the second column and then go to the last element of the third column and reduce it to $1$. Let's do both:

$\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & 1 & 6 & 3\\
    0 & -1 & -2 & -1
\end{array}\right]\xrightarrow{R_3 + R_2 \to R_3}
\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & 1 & 6 & 3\\
    0 & 0 & 4 & 2
\end{array}\right]\xrightarrow{\frac{1}{4}R_3 \to R_3}
\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & 1 & 6 & 3\\
    0 & 0 & 1 & \frac{1}{2}
\end{array}\right]
$
---
*Just like with Rubik's cube... if you follow the algorithm, slowly but steady, you start to see the solution appearing.*
---
By now, we already can see from the last row, that we found the value for $x_3$!

To get the entire solution vector $\mathbf{x}$, let's reduce the left side to the identity matrix!!

*One can only love the identity matrix...*

Let's continue using EROs to get to the identity matrix:

$\left[\begin{array}{ccc|c}  
    1 & 2 & 3 & 2\\  
    0 & 1 & 6 & 3\\
    0 & 0 & 1 & \frac{1}{2}
\end{array}\right]\xrightarrow{R_1 - 2 R_2 \to R_1}
\left[\begin{array}{ccc|c}  
    1 & 0 & -9 & -4\\  
    0 & 1 & 6 & 3\\
    0 & 0 & 1 & \frac{1}{2}
\end{array}\right]\xrightarrow{R_1 + 9 R_3 \to R_1}
\left[\begin{array}{ccc|c}  
    1 & 0 & 0 & \frac{1}{2}\\  
    0 & 1 & 6 & 3\\
    0 & 0 & 1 & \frac{1}{2}
\end{array}\right]\xrightarrow{R_2 - 6 R_3 \to R_2}
\left[\begin{array}{ccc|c}  
    1 & 0 & 0 & \frac{1}{2}\\  
    0 & 1 & 0 & 0\\
    0 & 0 & 1 & \frac{1}{2}
\end{array}\right]$
That's it! We've found our identity matrix, and therefore the solution vector $\mathbf{x}$!! Beautiful. üòç

Rewriting this as a system of linear equations, the solution becomes:

$\left\{
    \begin{align*}
        x_1 = \frac{1}{2}\\ 
        x_2 = 0\;\\ 
        x_3 = \frac{1}{2}
    \end{align*}
\right.$
From the 3 types of systems we saw before, this one corresponds to a **system with a unique solution**.
---
üôã *Spotted an error in the calculations? Awesome, [let your instructor know](https://github.com/LDSSA/ds-prep-course-2022/issues/new).*
---
I'm tired of writing so many matrices... your turn to do the work!! üòÅ
> üìù **Pen and paper exercise 5**: Using the solution we found above, check that the result of $\mathbf{A}\mathbf{x}$ matches the expected $\begin{bmatrix}2\\3\\1\end{bmatrix}$.
### 4.4 The inverse was there all along
**Think about this for a minute:** the operations we did on $\mathbf{A}$ resulted in the identity matrix, $\mathbf{I}$. We know that if a matrix $n\times n$ is invertible, then $\mathbf{A}\mathbf{A}^{-1} = I_n$.

So, under the hood, we were just performing linear operations that allowed us to transform $\mathbf{A}$ into its inverse $\mathbf{A}^{-1}$.
---
<img src="./media/shocked.gif" width="400"/>
---
```
(We are shaking some tombs here...)
    Jordan: -"Oh you think that's interesting? You don't know half the story..."
    Gauss: -"Is it 'your' story though?"
    Chinese mathematicians: -"You're all a bunch of copycats."
    Newton: -"Here here, have an apple."
```

[A bit of history never hurts.](https://en.wikipedia.org/wiki/Gaussian_elimination#History)
---
**Using Gauss-Jordan elimination to find the inverse:**
To find the inverse which was hidden all along throughout our Gauss-Jordan elimination, we simply need to insert the identity matrix into our augmented matrix, and include it in the ERO steps:

$$\left[\begin{array}{ccc|ccc|c}  
    1 & 2 & 3 & 1 & 0 & 0 & 2\\  
    4 & 1 & 2 & 0 & 1 & 0 & 3\\
    1 & 1 & 1 & 0 & 0 & 1 & 1
\end{array}\right]$$
Performing Gauss-Jordan elimination like we did before, we would have:

$\left[\begin{array}{ccc|ccc|c}  
    1 & 2 & 3 & 1 & 0 & 0 & 2\\  
    4 & 1 & 2 & 0 & 1 & 0 & 3\\
    1 & 1 & 1 & 0 & 0 & 1 & 1
\end{array}\right]\xrightarrow{R_2 - 4R_1\to R_2}
\left[\begin{array}{ccc|ccc|c}  
    1 & 2 & 3 & 1 & 0 & 0 & 2\\  
    0 & -7 & -10 & -4 & 1 & 0 & -5\\
    1 & 1 & 1 & 0 & 0 & 1 & 1
\end{array}\right]$

$\xrightarrow{}\dots\xrightarrow{}
\left[\begin{array}{ccc|ccc|c}  
    1 & 0 & 0 & -\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{2}\\  
    0 & 1 & 0 & -\frac{1}{2} & -\frac{1}{2} & \frac{5}{2} & 0\\
    0 & 0 & 1 & \frac{3}{4} & \frac{1}{4} & -\frac{7}{4} & \frac{1}{2}
\end{array}\right] = 
\mathbf{I} | \mathbf{A}^{-1} | \mathbf{x}
$

So the inverse of $\mathbf{A}$ is:

$\mathbf{A}^{-1} = 
\begin{bmatrix}
    -\frac{1}{4} & \frac{1}{4} & \frac{1}{4}\\  
    -\frac{1}{2} & -\frac{1}{2} & \frac{5}{2}\\
    \frac{3}{4} & \frac{1}{4} & -\frac{7}{4}
\end{bmatrix}$

We didn't have to include the last column, in order to find $\mathbf{A}^{-1}$. But now you see how to get the inverse and the solution (if they exist) at the same time!
---
‚ùóÔ∏è **Key takeaways:**

> - If we had used any other vector $\mathbf{b}$ in $\mathbf{A}|\mathbf{b}$, we would still be able to find some $\mathbf{x}$ to solve the equation, and the inverse of $\mathbf{A}$ would always be the same.
>
> The fact that we can put any 3D vector in the place of $\mathbf{b}$, and still find an $\mathbf{x}$, stems from the fact that the $3\times 3$ square matrix $\mathbf{A}$ is invertible, which is due to its column vectors all being linearly independent (they can represent all 3D vectors through linear combinations).
>
> - If there was no way to transform our left matrix into the identity matrix using the row reduction process, we would say that $\mathbf{A}$ has no inverse, or that it is **singular**.
---
> üìù **Pen and paper exercise 6**: Perform Gauss-Jordan elimination to find the solution, **if it exists**, and the inverse of $\mathbf{A}$ (**if it exists**), of the following system:
>
> $\;\;\mathbf{A}\mathbf{x} = \mathbf{b} \iff
\begin{bmatrix}
        2 & 1 & -1\\
        1 & -3 & 2\\
        1 & 4 & -3
    \end{bmatrix}
    \mathbf{x} = 
    \begin{bmatrix}
        7\\
        1\\
        5
    \end{bmatrix}$
>
> What type of system is this? (unique solution, no solution, or infinite number of solutions?)
>
> Remember you can use any of the [allowed EROs we've learned about](#Elementary-row-operations-(EROs):).
### 4.5 Matrix invertibility *vs.* types of linear systems
For a linear system with $n$ equations and $n$ unknowns:

- If we have **a unique solution**, the square matrix $\mathbf{A}$ is **invertible**;
- If we have **an infinite range of solutions**, the square matrix $\mathbf{A}$ is **singular**;
- If we have **no solution**, the square matrix $\mathbf{A}$ is **singular** **and**, somewhere along the elimination steps, you'll get a row representing an **impossible equation** (something like $0=2$ or $0=-1$).
## Uncomment the lines below to check the inverse (if it exists) of A in exercise 6
#A = np.array([[2, 1, -1], [1, -3, 2], [1, 4, -3]])
#np.linalg.inv(A)
---
<img src="./media/no_time.png"/>
---
### 4.6 Solving linear equations in NumPy
Let's see if we can solve the same system we did before, with a few lines of code and NumPy's amazing collection of linear algebra functions, [`numpy.linalg`](https://numpy.org/doc/1.20/reference/routines.linalg.html).

$\left\{
    \begin{align*}
        x_1 + 2x_2 + 3x_3 = 2\\ 
        4x_1 + x_2 + 2x_3 = 3\\ 
        x_1 + x_2 + x_3 = 1
    \end{align*}
\right.$
---
**1 - (*super lazy method*) Use [`numpy.linalg.solve()`](https://numpy.org/doc/1.20/reference/generated/numpy.linalg.solve.html):**
# matrix of coefficients A
A = np.array([[1,2,3], [4,1,2], [1,1,1]])  # 2D array
# colum nvector b
b = np.array([[2], [3], [1]])  # 2D array

np.linalg.solve(A, b)
Exactly what we got when solving with Gauss-Jordan elimination!! *Just a hundred times faster...* üòÖüòÖ
---
**2 - (*not so lazy method*) Use `numpy.linalg.inv()` to find the inverse and then solve for $\mathbf{x}$:**
We can rearrange the equation $\mathbf{A}\mathbf{x} = \mathbf{b}$ by multiplying both sides of the equation by $\mathbf{A}^{-1}$: $\;\;\;\;\mathbf{A}^{-1}\mathbf{A}\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$

We know that, **if $\mathbf{A}$ is invertible, then $\mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$**, so we can simplify this equation to: $\;\;\;\;\mathbf{I}\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$

We also know that the identity matrix does not change another matrix (or vector) when multiplied by it, therefore we can write: $\;\;\;\;\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$
After having checked if $\mathbf{A}$ is invertible (which we do using `np.linalg.inv()`), we apply this equation in NumPy and get the vector $\mathbf{x}$:
# If A is invertible, Ax = b => x = A^{-1} b

# our matrix A (3x3)
A = np.array([[1,2,3], 
              [4,1,2], 
              [1,1,1]])

# our vector b (3x1)
b = np.array([[2],
              [3],
              [1]])

# the inverse of A (3x3)
A_inv = np.linalg.inv(A)

# the solution vector x (3x1)
x = np.matmul(A_inv, b)

# print solution
print("x = ", x)
Yep, that's the expected result.
---
**For the math nerds**:

Check [this video](https://youtu.be/J7DzL2_Na80?list=PL221E2BBF13BECF6C) if you're curious to learn about the geometry of linear equations.
---
<img src="./media/almost.gif" width="400"/>

<center>Only one more section to go...</center>
---
## 5. Eigenvalues and eigenvectors
### (optional section)
The words **eigenvalue** and **eigenvector** come from the german *Eigen*, meaning "own", or "characteristic":
<a name="vector_def"></a>
<div class="alert alert-block alert-info">
    If we obtain a scaled version of the <b>non-zero</b> vector $\mathbf{x}$, $\;\;\lambda\mathbf{x}\;$, when  multiplying a square matrix $\mathbf{A}$ by that vector, $\;\;\mathbf{A}\mathbf{x}\;$, then we say that $\mathbf{x}$ is an <b>eigenvector</b> and $\lambda$ is an <b>eigenvalue</b>:
$$\mathbf{A}\mathbf{x} = \lambda\mathbf{x}$$
</div>

Think about what this means: it means that by using only 1 scalar, you can get the same vector as using one entire matrix (potentially huge). It's as if you're "magically" reducing the number of dimensions in your data matrix, thus simplifying your problem, thus taking away a lot of computational burden.

So, reducing the dimensionality of our data seems pretty useful... but how do we find those 2 unknowns, $\mathbf{x}$ and $\lambda$?
### 5.1 Enter the determinant
To be able to discover eigenvectors $\mathbf{x}\neq 0$ and eigenvalues $\lambda$, we need to first talk about the determinant.

The **determinant** of $\mathbf{A}$, denoted as $\det(\mathbf{A})$, or simply $|\mathbf{A}|$, is a special number that tells us **whether** a **square matrix** is **invertible** or **singular**.

For our purposes here, all you need to know is:

- If the determinant of a square matrix $\mathbf{A}$ is zero, $\det(\mathbf{A}) = 0$, then $\mathbf{A}$ is **singular**;
- If the determinant of a square matrix $\mathbf{A}$ is different from zero, $\det(\mathbf{A}) \neq 0$, then $\mathbf{A}$ is **invertible**.

Making the bridge to what we learned about systems of linear equations, we can also say that, for a square matrix $\mathbf{A}$, of size $n\times n$:

- If $\det(\mathbf{A}) = 0$, $\mathbf{A}$ is **singular** and the system **either** has **no solution**, or **an infinite number of solutions**;
- If $\det(\mathbf{A}) \neq 0$, $\mathbf{A}$ is **invertible** and the system has **a unique solution**.
#### 5.1.1 Calculating the determinant with NumPy
Let's use [`numpy.linalg.det()`](https://numpy.org/doc/1.20/reference/generated/numpy.linalg.det.html) to find the determinant of a matrix for us.
**(i) Determinant of a singular matrix**
# a singular matrix --> det(A) = 0
A = np.array([[1, 2],
              [2, 4]])
np.linalg.det(A)
**(ii) Determinant of an invertible matrix**
# an invertible matrix --> det(B) not equal to 0
B = np.array([[2, -1, 6],
              [1, 1, 2],
              [3, 1, 2]])
np.linalg.det(B).round(2)
The inverse of `B` is:
# inverse of matrix B
np.linalg.inv(B)
**(iii) Determinant of the identity matrix, also an invertible matrix**
# the identity matrix is invertible --> determinant not equal to 0
I = np.array([[1, 0, 0],
              [0, 1, 0],
              [0, 0, 1]])
np.linalg.det(I)
The determinant of the identity matrix is always $1$. Unsurprisingly, the inverse of the identity matrix `I` is the identity matrix itself:
np.linalg.inv(I)
Honestly, who can resist the identity matrix?...
---

#### Oh and by the way, did you choose your [favourite matrix](https://www.youtube.com/watch?v=1DvlyNjfuzM) already?

<img src="./media/cupcakematrix.jpg" width="600"/>
<center>Time for matrix cupcakes!! :D</center>
---
### 5.2 Finding eigenvalues and eigenvectors
Let's bring back the equation for the eigenvectors and eigenvalues, $\mathbf{A}\mathbf{x} = \lambda\mathbf{x}$.

We can rearrange the equation to:

$$\mathbf{A}\mathbf{x}-\lambda\mathbf{x} = 0$$

Remember that we can write $\mathbf{I}\mathbf{x} = \mathbf{x}$, where $\mathbf{I}$ is the identity matrix:

$$\mathbf{A}\mathbf{x}-\lambda\mathbf{I}\mathbf{x} = 0$$

We can apply the multiplication properties we've learned on vectors, scalars and matrices and write:

$$(\mathbf{A}-\lambda\mathbf{I})\mathbf{x} = 0$$
Notice that what we have here is a system of linear equations, of the form $\mathbf{B}\mathbf{x} = \mathbf{b}$, where our vector $\mathbf{b}$ corresponds to the zero vector, and our matrix $\mathbf{B}$ corresponds to $\mathbf{A}-\lambda\mathbf{I}$. Indeed, for an $n\times n$ matrix $\mathbf{A}$, we could write this as a system of linear equations as follows:

$\left\{
    \begin{array}{r@r}
        (a_{1,1} - \lambda)x_1 & + &
            a_{1,2}x_2 & + &
            \dots & + &
            a_{1,n}x_n & = &
            0\\ 
        a_{2,1}x_1 & + &
            (a_{2,2}-\lambda)x_2 & + &
            \dots & + &
            a_{2,n}x_n & = & 
            0\\ 
            \vdots&&\vdots&&\ddots&&\vdots&&\vdots\\
        a_{n,1}x_1 & + &
            a_{n,2}x_2 & + &
            \dots & + &
            (a_{n,n} - \lambda)x_n & = & 0
    \end{array}
\right.$

The zero vector is obviously a solution to this system, **however**, from the definition of eigenvector, what we really want is some vector $\mathbf{x}\neq 0$.
---
Because $\lambda$ is unknown, we don't yet know what's in the matrix $\mathbf{A}-\lambda\mathbf{I}$. But since it is a square matrix, we know it either is invertible (thus it has nonzero determinant) or singular (thus having zero determinant).

If it is invertible, then the system can only have one solution, and that would be the zero vector. We therefore need it to be singular, so that we can find some other vector than the zero one. So we need to be able to find $\lambda$ for which the determinant is zero, $\det(\mathbf{A}-\lambda\mathbf{I})=0$, which we can rewrite as:

$$\left|\begin{array}
    a_{1,1}-\lambda & a_{1,2} & \dots & a_{1,n}\\
    a_{2,1} & a_{2,2}-\lambda & \dots & a_{2,n}\\
    \vdots & \vdots & \ddots & \vdots\\
    a_{n,1} & a_{n,2} & \dots & a_{n,n}-\lambda
\end{array}\right| = 0
$$

The calculation of the determinant for an $n$-dimensional square matrix is out of scope, however just know that to solve the equation above you would get a **polynomial of degree $n$ on the variable $\lambda$**.

After having each of the eigenvalues $\lambda_1, \lambda_2,...$ that solve the polynomial equation, one can find the corresponding eigenvectors by solving the linear system, for each of the eigenvalues found.
**Number of eigenvalues for an $n\times n$ square matrix:**

- For a $2\times 2$ matrix, you can get at most 2 different eigenvalues;

- For a $3\times 3$ singular matrix, you could find at most 3 different eigenvalues;

And so on.
---
Although we won't go into further details, let's just check a very simple case, where we have a triangular matrix:

$$\left|\begin{array}
    a_{1,1}-\lambda & a_{1,2} & \dots & a_{1,n}\\
    0 & a_{2,2}-\lambda & \dots & a_{2,n}\\
    \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \dots & a_{n,n}-\lambda
\end{array}\right| = 0
$$

In this case, our polynomial equation would come down to:

$$(a_{1,1}-\lambda)(a_{2,2}-\lambda)\dots(a_{n,n}-\lambda) = 0$$

Thus, in this particular case, the eigenvalues of $\mathbf{A}$ are just the entries of the diagonal of $\mathbf{A}$.
---
You can check a visual explanation of **Eigenvectors and Eigenvalues** [here](https://setosa.io/ev/eigenvectors-and-eigenvalues/).
---
<img src="./media/thats_a_lot_of_math.gif"/>

<br>

<br>

When working with lots and lots of data, we don't have the time to perform all those calculations by hand... so let's put NumPy to work and compute all the eigen-stuff.
---
### 5.3 Finding eigenvalues and eigenvectors with NumPy
We can find the eigenvectors and eigenvalues of a matrix using the function [`numpy.linalg.eig()`](https://numpy.org/doc/1.20/reference/generated/numpy.linalg.eig.html). Just a small snippet of code, to do all that math...
`numpy.linalg.ein()` takes a **square** array as input and returns:
- an array `w` with its eigenvalues;
- the normalized (unit ‚Äúlength‚Äù) eigenvectors, such that the column `v[:,i]` is an eigenvector corresponding to the eigenvalue `w[i]`.
# create a 3x3 matrix using a 2D numpy array
A = np.array([[1, 2, 3],
              [3, -5, 3],
              [6, -6, 4]])

# determine the eigenvalues and eigenvectors of matrix A
eigenvalues, eigenvectors = np.linalg.eig(A)

# display eigenvalues and eigenvectors
print("eigenvalues of A:", eigenvalues, "\n")
print("eigenvectors of A:\n", eigenvectors, "\n")
The function `numpy.linalg.eig()` might return the eigenvalues and eigenvector entries as complex numbers....
It is indeed possible that you find eigenvalues and eigenvectors with an **imaginary** part. Remember that, to find the $\lambda$ for which $\det(\mathbf{A}-\lambda\mathbf{I})=0$, one usually needs to solve a polynomial equation, and some polynomial equations actually have **complex roots**.
---
<img src="./media/key_of_imagination.gif">
---
# create a 4x4 matrix using a 2D numpy array
B = np.array([[2, 0, 0, 0],
              [0, 2, 0, 0],
              [0, 0, 5, 0],
              [0, 0, 0, 4]])

# determine the eigenvalues and eigenvectors of square matrix B
B_eigenvalues, B_eigenvectors = np.linalg.eig(B)

# display eigenvalues and eigenvectors
print("eigenvalues of B:", B_eigenvalues, "\n")
print("eigenvectors of B:\n", B_eigenvectors, "\n")
# create a 3x3 matrix using a 2D numpy array
C = np.array([[2, 4, 0],
              [4, 2, 0],
              [0, 0, 5]])

# determine the eigenvalues and eigenvectors of square matrix C
C_eigenvalues, C_eigenvectors = np.linalg.eig(C)

# display eigenvalues and eigenvectors
print("eigenvalues of C:", C_eigenvalues, "\n")
print("eigenvectors of C:\n", C_eigenvectors, "\n")
---
<img src="./media/mathematical_enlightenment.gif"/>

<br>

<center>This is it. You have reached the first stage of mathematical enlightenment.</center>

<center>Your life will never be the same again.</center>

<br>

<center><font size=4>üò≤üò≤</font></center>

<center>Well you chose, mathematically enlightened you've become.</center>
---
<br>

<br>

<br>

<center><font size=4>End of optional sections</font></center>

<br>

<br>

<br>
---
### Reading the matrix form of the multiple linear regression solution
At the beginning of SLU12, I told you you'd be able to read the matrix form of the general solution to the multiple [linear regression](https://en.wikipedia.org/wiki/Linear_regression) algorithm. Let's do this!

In linear regression, you want to find some function to relate an outcome $\mathbf{y}$ to other phenomena, or variables, such as $\mathbf{x_1}, \mathbf{x_2}, \dots, \mathbf{x_n}$, using a linear relation:

$$\mathbf{y} = \beta_1\mathbf{x_1} + \dots + \beta_n\mathbf{x_n}$$

You have a set of data (usually large) $\mathbf{X}$, a matrix where you store all your observation variables, and $\mathbf{y}$, a vector with the corresponding outcomes:

$$\begin{array}{c|cccc}
y&x_1&x_2&\dots&x_n\\
\hline
y_1&x_{1,1}&x_{1,2}&\dots&x_{1,n}\\
y_2&x_{2,1}&x_{2,2}&\dots&x_{2,n}\\
\dots&\dots&\dots&\dots&\dots\\
y_m&x_{m,1}&x_{m,2}&\dots&x_{m,n}\\
\end{array}$$

 You want to find the values of the **coefficients** $\beta_1, \dots, \beta_n$, so that when you have a new observation, you can predict $\mathbf{y}$. But how do we find the "right" values of the coefficients?

We want the following equalities to be satisfied, *as much as possible*, all at once:

$$\begin{array}{l}
y_1=\beta_1x_{1,1}+\beta_2x_{1,2}+\dots+\beta_nx_{1,n}\\
y_2=\beta_1x_{2,1}+\beta_2x_{2,2}+\dots+\beta_nx_{2,n}\\
\qquad \qquad \dots\\
y_m=\beta_1x_{m,1}+\beta_2x_{m,2}+\dots+\beta_nx_{m,n}\end{array}$$

<br>

---

This is a system of linear equations, thus we can write it in the form:

$$\mathbf{X}\mathbf{\beta} = \mathbf{y}$$

**Notice that** now, our $\mathbf{A}$ is called $\mathbf{X}$, our $\mathbf{x}$ is now the $\mathbf{\beta}$ and our $\mathbf{b}$ is now called $\mathbf{y}$.

---

Actually, you can't find a *perfect* solution to this in real life. What you'll actually do is try to find a *solution* that is as good as possible. And Mathematics tells us that the best possible solution is given by the equality:

$$\mathbf{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{y})$$

provided that the columns in $\mathbf{X}$ are linearly independent. That's right, because now you know that $\mathbf{X}^T\mathbf{X}$ should be non singular for the inverse to exist!!

---

‚ö†Ô∏è **Disclaimer:** This is **a simplistic view** of the multiple linear regression algorithm. But as you become more and more comfortable with linear algebra, the easier it will be to read and make sense of matrix equations.

<br>

```
    Some senior data scientist: -"Thank you for that disclaimer."
    Instructor: -"No problem."
    Student: -"Hey guys, will I become insane like all the Mathematicians out there, if I do data science?"
    Instructor: -"You will, but only if you do it the right way..."
```
---
<img src="./media/math_religion.png" width="750"/>
---
## Wrapping up
What we've learned in this notebook:
- how to compute the product between two matrices, or a matrix and a vector;
- how to determine the inverse of a matrix using NumPy;
- useful `ndarray` methods: `.min()`, `.max()`, `.sum()` and `.sort()`;
- (for the ones that opted in) solving systems of linear equations and finding the inverse of a matrix, eigenvalues and eigenvectors.
---
### Resources on Linear Algebra:

- [**3Blue1Brown**](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) YouTube Playlist with an intuitive and visual introduction to Linear Algebra;

- [**YouTube Playlist for MIT 18.06SC Linear Algebra, Fall 2011**](https://www.youtube.com/watch?v=7UJ4CFRGd-U&list=PL221E2BBF13BECF6C);


### Resources on NumPy:

* [**NumPy v1.26 Quickstart tutorial**](https://numpy.org/doc/1.20/user/quickstart.html) NumPy's official tutorial;
---